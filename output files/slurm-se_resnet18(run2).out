Running SLURM prolog script on indigo52.cluster.local
===============================================================================
Job started on Fri Sep 10 14:08:10 BST 2021
Job ID          : 468613
Job name        : run_on_iridis_SE.sh
WorkDir         : /mainfs/scratch/jz1n20/SE_Resnet18
Command         : /mainfs/scratch/jz1n20/SE_Resnet18/run_on_iridis_SE.sh
Partition       : gpu
Num hosts       : 1
Num cores       : 10
Num of tasks    : 10
Hosts allocated : indigo52
Job Output Follows ...
===============================================================================
# conda environments:
#
DeepL                 *  /home/jz1n20/.conda/envs/DeepL
env1                     /home/jz1n20/.conda/envs/env1
base                     /local/software/conda/miniconda-py3-new
serviceline              /local/software/conda/miniconda-py3-new/envs/serviceline

==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
==> Model structure: 
SEResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): SEBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=64, out_features=4, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=4, out_features=64, bias=False)
          (3): Sigmoid()
        )
      )
    )
    (1): SEBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=64, out_features=4, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=4, out_features=64, bias=False)
          (3): Sigmoid()
        )
      )
    )
  )
  (layer2): Sequential(
    (0): SEBasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=128, out_features=8, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=8, out_features=128, bias=False)
          (3): Sigmoid()
        )
      )
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): SEBasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=128, out_features=8, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=8, out_features=128, bias=False)
          (3): Sigmoid()
        )
      )
    )
  )
  (layer3): Sequential(
    (0): SEBasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=256, out_features=16, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=16, out_features=256, bias=False)
          (3): Sigmoid()
        )
      )
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): SEBasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=256, out_features=16, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=16, out_features=256, bias=False)
          (3): Sigmoid()
        )
      )
    )
  )
  (layer4): Sequential(
    (0): SEBasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=512, out_features=32, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=32, out_features=512, bias=False)
          (3): Sigmoid()
        )
      )
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): SEBasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SELayer(
        (avg_pool): AdaptiveAvgPool2d(output_size=1)
        (fc): Sequential(
          (0): Linear(in_features=512, out_features=32, bias=False)
          (1): ReLU(inplace=True)
          (2): Linear(in_features=32, out_features=512, bias=False)
          (3): Sigmoid()
        )
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=10, bias=True)
)

Epoch: 0
tr_accuracy : 39.668, tr_loss : 682.5745495557785
/home/jz1n20/.conda/envs/DeepL/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Saving..
test_accuracy : 47.93, test_loss : 113.59162020683289, best_accuracy: 47.93

Epoch: 1
tr_accuracy : 53.676, tr_loss : 507.9431117773056
Saving..
test_accuracy : 55.64, test_loss : 105.70064842700958, best_accuracy: 55.64

Epoch: 2
tr_accuracy : 59.972, tr_loss : 442.70784217119217
Saving..
test_accuracy : 61.02, test_loss : 89.02546048164368, best_accuracy: 61.02

Epoch: 3
tr_accuracy : 63.98, tr_loss : 400.3662938475609
Saving..
test_accuracy : 66.0, test_loss : 77.9888145327568, best_accuracy: 66.0

Epoch: 4
tr_accuracy : 66.556, tr_loss : 375.2797790169716
Saving..
test_accuracy : 68.99, test_loss : 71.23643720149994, best_accuracy: 68.99

Epoch: 5
tr_accuracy : 68.054, tr_loss : 359.40280961990356
test_accuracy : 67.66, test_loss : 74.56289839744568, best_accuracy: 68.99

Epoch: 6
tr_accuracy : 69.514, tr_loss : 341.1797034740448
test_accuracy : 67.21, test_loss : 74.14656287431717, best_accuracy: 68.99

Epoch: 7
tr_accuracy : 70.644, tr_loss : 331.44280952215195
Saving..
test_accuracy : 72.56, test_loss : 62.94401431083679, best_accuracy: 72.56

Epoch: 8
tr_accuracy : 71.8, tr_loss : 320.083181142807
test_accuracy : 67.63, test_loss : 73.26636129617691, best_accuracy: 72.56

Epoch: 9
tr_accuracy : 72.462, tr_loss : 311.8282080888748
test_accuracy : 70.5, test_loss : 68.32743689417839, best_accuracy: 72.56

Epoch: 10
tr_accuracy : 72.448, tr_loss : 311.69642156362534
test_accuracy : 70.5, test_loss : 69.69061094522476, best_accuracy: 72.56

Epoch: 11
tr_accuracy : 72.926, tr_loss : 304.03705406188965
Saving..
test_accuracy : 72.87, test_loss : 62.02744972705841, best_accuracy: 72.87

Epoch: 12
tr_accuracy : 73.708, tr_loss : 298.1932289004326
test_accuracy : 67.04, test_loss : 79.11260211467743, best_accuracy: 72.87

Epoch: 13
tr_accuracy : 74.016, tr_loss : 294.85304123163223
Saving..
test_accuracy : 73.77, test_loss : 60.41487658023834, best_accuracy: 73.77

Epoch: 14
tr_accuracy : 74.31, tr_loss : 290.09447851777077
test_accuracy : 70.17, test_loss : 69.37765139341354, best_accuracy: 73.77

Epoch: 15
tr_accuracy : 74.494, tr_loss : 288.55710366368294
test_accuracy : 70.02, test_loss : 68.85948574542999, best_accuracy: 73.77

Epoch: 16
tr_accuracy : 75.146, tr_loss : 284.1782526373863
test_accuracy : 73.37, test_loss : 62.10261523723602, best_accuracy: 73.77

Epoch: 17
tr_accuracy : 74.78, tr_loss : 285.3857771754265
test_accuracy : 70.27, test_loss : 68.80245929956436, best_accuracy: 73.77

Epoch: 18
tr_accuracy : 75.64, tr_loss : 276.86718064546585
test_accuracy : 71.87, test_loss : 64.86945676803589, best_accuracy: 73.77

Epoch: 19
tr_accuracy : 75.424, tr_loss : 279.69402036070824
test_accuracy : 70.79, test_loss : 70.19231641292572, best_accuracy: 73.77

Epoch: 20
tr_accuracy : 75.454, tr_loss : 280.0218046307564
test_accuracy : 71.88, test_loss : 65.22402757406235, best_accuracy: 73.77

Epoch: 21
tr_accuracy : 75.76, tr_loss : 275.92542004585266
test_accuracy : 71.86, test_loss : 67.24840301275253, best_accuracy: 73.77

Epoch: 22
tr_accuracy : 75.662, tr_loss : 275.5302742123604
test_accuracy : 69.41, test_loss : 68.6889335513115, best_accuracy: 73.77

Epoch: 23
tr_accuracy : 75.94, tr_loss : 274.27372819185257
Saving..
test_accuracy : 75.49, test_loss : 57.8432237803936, best_accuracy: 75.49

Epoch: 24
tr_accuracy : 76.062, tr_loss : 271.9376949071884
test_accuracy : 74.25, test_loss : 58.5617356300354, best_accuracy: 75.49

Epoch: 25
tr_accuracy : 76.282, tr_loss : 268.7487357854843
test_accuracy : 70.81, test_loss : 68.35980254411697, best_accuracy: 75.49

Epoch: 26
tr_accuracy : 76.25, tr_loss : 270.0189430117607
test_accuracy : 73.55, test_loss : 62.554208278656006, best_accuracy: 75.49

Epoch: 27
tr_accuracy : 76.424, tr_loss : 267.8892144560814
test_accuracy : 71.45, test_loss : 66.98509341478348, best_accuracy: 75.49

Epoch: 28
tr_accuracy : 76.532, tr_loss : 269.56414556503296
test_accuracy : 74.89, test_loss : 58.12257021665573, best_accuracy: 75.49

Epoch: 29
tr_accuracy : 76.984, tr_loss : 264.20917892456055
test_accuracy : 71.53, test_loss : 67.30354523658752, best_accuracy: 75.49

Epoch: 30
tr_accuracy : 76.474, tr_loss : 268.2963425517082
Saving..
test_accuracy : 76.14, test_loss : 54.18789201974869, best_accuracy: 76.14

Epoch: 31
tr_accuracy : 77.122, tr_loss : 261.4835202395916
test_accuracy : 74.38, test_loss : 60.63788777589798, best_accuracy: 76.14

Epoch: 32
tr_accuracy : 77.008, tr_loss : 263.1373027265072
test_accuracy : 74.44, test_loss : 58.52069544792175, best_accuracy: 76.14

Epoch: 33
tr_accuracy : 76.948, tr_loss : 263.6691982448101
test_accuracy : 75.37, test_loss : 57.30355906486511, best_accuracy: 76.14

Epoch: 34
tr_accuracy : 76.846, tr_loss : 264.6059225797653
test_accuracy : 73.98, test_loss : 58.95391893386841, best_accuracy: 76.14

Epoch: 35
tr_accuracy : 77.182, tr_loss : 261.33217734098434
test_accuracy : 75.26, test_loss : 58.025591135025024, best_accuracy: 76.14

Epoch: 36
tr_accuracy : 77.28, tr_loss : 258.8673816919327
test_accuracy : 72.85, test_loss : 63.31317067146301, best_accuracy: 76.14

Epoch: 37
tr_accuracy : 76.974, tr_loss : 263.64201334118843
test_accuracy : 71.35, test_loss : 68.85360497236252, best_accuracy: 76.14

Epoch: 38
tr_accuracy : 77.06, tr_loss : 261.35815557837486
Saving..
test_accuracy : 76.45, test_loss : 53.852357774972916, best_accuracy: 76.45

Epoch: 39
tr_accuracy : 77.306, tr_loss : 259.26086857914925
test_accuracy : 74.43, test_loss : 59.21700698137283, best_accuracy: 76.45

Epoch: 40
tr_accuracy : 77.282, tr_loss : 260.38868167996407
test_accuracy : 75.62, test_loss : 55.56031706929207, best_accuracy: 76.45

Epoch: 41
tr_accuracy : 77.394, tr_loss : 258.9807565808296
test_accuracy : 74.87, test_loss : 58.36197489500046, best_accuracy: 76.45

Epoch: 42
tr_accuracy : 77.484, tr_loss : 257.43612653017044
test_accuracy : 74.26, test_loss : 59.507386445999146, best_accuracy: 76.45

Epoch: 43
tr_accuracy : 77.628, tr_loss : 255.7533193230629
test_accuracy : 76.27, test_loss : 54.51874914765358, best_accuracy: 76.45

Epoch: 44
tr_accuracy : 77.716, tr_loss : 256.19497433304787
Saving..
test_accuracy : 77.24, test_loss : 52.297276228666306, best_accuracy: 77.24

Epoch: 45
tr_accuracy : 77.66, tr_loss : 255.4206364750862
test_accuracy : 73.87, test_loss : 60.1464484333992, best_accuracy: 77.24

Epoch: 46
tr_accuracy : 77.44, tr_loss : 256.51995554566383
test_accuracy : 75.87, test_loss : 56.621676564216614, best_accuracy: 77.24

Epoch: 47
tr_accuracy : 77.442, tr_loss : 257.0632509291172
test_accuracy : 75.13, test_loss : 57.98396024107933, best_accuracy: 77.24

Epoch: 48
tr_accuracy : 77.87, tr_loss : 254.17515921592712
test_accuracy : 75.52, test_loss : 56.85489109158516, best_accuracy: 77.24

Epoch: 49
tr_accuracy : 77.692, tr_loss : 254.92703792452812
test_accuracy : 75.19, test_loss : 58.30605357885361, best_accuracy: 77.24

Epoch: 50
tr_accuracy : 77.794, tr_loss : 253.94039419293404
test_accuracy : 73.17, test_loss : 63.88274556398392, best_accuracy: 77.24

Epoch: 51
tr_accuracy : 77.894, tr_loss : 253.44110614061356
test_accuracy : 71.83, test_loss : 66.16008347272873, best_accuracy: 77.24

Epoch: 52
tr_accuracy : 77.904, tr_loss : 252.7337241768837
test_accuracy : 76.95, test_loss : 54.3178434073925, best_accuracy: 77.24

Epoch: 53
tr_accuracy : 78.144, tr_loss : 251.45462730526924
test_accuracy : 74.9, test_loss : 58.75996953248978, best_accuracy: 77.24

Epoch: 54
tr_accuracy : 78.428, tr_loss : 248.60900983214378
test_accuracy : 75.54, test_loss : 57.139211893081665, best_accuracy: 77.24

Epoch: 55
tr_accuracy : 77.942, tr_loss : 251.80333206057549
test_accuracy : 75.27, test_loss : 56.46654886007309, best_accuracy: 77.24

Epoch: 56
tr_accuracy : 78.116, tr_loss : 249.16728001832962
test_accuracy : 74.66, test_loss : 59.73605763912201, best_accuracy: 77.24

Epoch: 57
tr_accuracy : 78.1, tr_loss : 250.62827908992767
test_accuracy : 74.49, test_loss : 58.83529579639435, best_accuracy: 77.24

Epoch: 58
tr_accuracy : 78.276, tr_loss : 249.23340886831284
test_accuracy : 74.68, test_loss : 57.02795892953873, best_accuracy: 77.24

Epoch: 59
tr_accuracy : 78.298, tr_loss : 248.24612826108932
Saving..
test_accuracy : 78.2, test_loss : 50.62783023715019, best_accuracy: 78.2

Epoch: 60
tr_accuracy : 78.266, tr_loss : 245.8141924738884
test_accuracy : 73.86, test_loss : 60.12579566240311, best_accuracy: 78.2

Epoch: 61
tr_accuracy : 78.358, tr_loss : 247.8098518550396
test_accuracy : 74.73, test_loss : 58.04787391424179, best_accuracy: 78.2

Epoch: 62
tr_accuracy : 78.172, tr_loss : 247.54410189390182
test_accuracy : 74.65, test_loss : 58.76065248250961, best_accuracy: 78.2

Epoch: 63
tr_accuracy : 78.466, tr_loss : 246.4666941165924
test_accuracy : 75.44, test_loss : 56.79367792606354, best_accuracy: 78.2

Epoch: 64
tr_accuracy : 78.354, tr_loss : 245.77287364006042
test_accuracy : 76.05, test_loss : 57.17866572737694, best_accuracy: 78.2

Epoch: 65
tr_accuracy : 78.596, tr_loss : 244.32004210352898
test_accuracy : 74.85, test_loss : 57.05202329158783, best_accuracy: 78.2

Epoch: 66
tr_accuracy : 78.462, tr_loss : 245.48587116599083
test_accuracy : 77.26, test_loss : 53.42427942156792, best_accuracy: 78.2

Epoch: 67
tr_accuracy : 78.578, tr_loss : 243.58669048547745
test_accuracy : 76.36, test_loss : 55.90348717570305, best_accuracy: 78.2

Epoch: 68
tr_accuracy : 78.892, tr_loss : 243.96340256929398
test_accuracy : 76.76, test_loss : 53.68053841590881, best_accuracy: 78.2

Epoch: 69
tr_accuracy : 78.804, tr_loss : 242.08698523044586
test_accuracy : 73.35, test_loss : 63.31546550989151, best_accuracy: 78.2

Epoch: 70
tr_accuracy : 78.824, tr_loss : 242.63802540302277
test_accuracy : 72.36, test_loss : 64.66166132688522, best_accuracy: 78.2

Epoch: 71
tr_accuracy : 78.892, tr_loss : 243.67281290888786
test_accuracy : 75.08, test_loss : 56.740922421216965, best_accuracy: 78.2

Epoch: 72
tr_accuracy : 78.684, tr_loss : 241.48661422729492
test_accuracy : 74.07, test_loss : 60.60912561416626, best_accuracy: 78.2

Epoch: 73
tr_accuracy : 78.994, tr_loss : 239.37512612342834
test_accuracy : 74.92, test_loss : 58.358359694480896, best_accuracy: 78.2

Epoch: 74
tr_accuracy : 78.746, tr_loss : 242.17191848158836
test_accuracy : 78.13, test_loss : 50.66754910349846, best_accuracy: 78.2

Epoch: 75
tr_accuracy : 78.85, tr_loss : 241.21085107326508
test_accuracy : 76.16, test_loss : 55.33001461625099, best_accuracy: 78.2

Epoch: 76
tr_accuracy : 79.266, tr_loss : 236.59151351451874
test_accuracy : 74.63, test_loss : 59.57427975535393, best_accuracy: 78.2

Epoch: 77
tr_accuracy : 78.926, tr_loss : 241.3196538090706
test_accuracy : 75.06, test_loss : 59.44434550404549, best_accuracy: 78.2

Epoch: 78
tr_accuracy : 79.076, tr_loss : 238.8594529926777
test_accuracy : 77.06, test_loss : 52.277360677719116, best_accuracy: 78.2

Epoch: 79
tr_accuracy : 79.174, tr_loss : 238.04747968912125
test_accuracy : 77.23, test_loss : 52.29324248433113, best_accuracy: 78.2

Epoch: 80
tr_accuracy : 79.152, tr_loss : 238.0882506966591
test_accuracy : 75.08, test_loss : 57.41767680644989, best_accuracy: 78.2

Epoch: 81
tr_accuracy : 79.38, tr_loss : 235.87526845932007
test_accuracy : 77.39, test_loss : 52.198075234889984, best_accuracy: 78.2

Epoch: 82
tr_accuracy : 79.23, tr_loss : 236.9178554713726
test_accuracy : 77.68, test_loss : 51.36254671216011, best_accuracy: 78.2

Epoch: 83
tr_accuracy : 79.354, tr_loss : 237.770453363657
test_accuracy : 76.83, test_loss : 53.18577802181244, best_accuracy: 78.2

Epoch: 84
tr_accuracy : 79.274, tr_loss : 237.07765471935272
test_accuracy : 76.52, test_loss : 54.728876888751984, best_accuracy: 78.2

Epoch: 85
tr_accuracy : 79.262, tr_loss : 236.56527695059776
test_accuracy : 76.49, test_loss : 54.07941660284996, best_accuracy: 78.2

Epoch: 86
tr_accuracy : 79.53, tr_loss : 234.10003569722176
Saving..
test_accuracy : 78.59, test_loss : 49.489612460136414, best_accuracy: 78.59

Epoch: 87
tr_accuracy : 79.296, tr_loss : 234.88773894309998
test_accuracy : 74.53, test_loss : 62.459468603134155, best_accuracy: 78.59

Epoch: 88
tr_accuracy : 79.706, tr_loss : 233.42845153808594
Saving..
test_accuracy : 79.16, test_loss : 47.48807284235954, best_accuracy: 79.16

Epoch: 89
tr_accuracy : 79.674, tr_loss : 233.9413793683052
test_accuracy : 71.86, test_loss : 66.64804244041443, best_accuracy: 79.16

Epoch: 90
tr_accuracy : 80.044, tr_loss : 230.4636070728302
Saving..
test_accuracy : 79.17, test_loss : 48.06385573744774, best_accuracy: 79.17

Epoch: 91
tr_accuracy : 79.886, tr_loss : 230.2301640510559
test_accuracy : 77.12, test_loss : 54.133612245321274, best_accuracy: 79.17

Epoch: 92
tr_accuracy : 79.83, tr_loss : 229.86120900511742
test_accuracy : 77.23, test_loss : 53.96313413977623, best_accuracy: 79.17

Epoch: 93
tr_accuracy : 79.944, tr_loss : 229.71037179231644
test_accuracy : 76.13, test_loss : 57.03151047229767, best_accuracy: 79.17

Epoch: 94
tr_accuracy : 79.914, tr_loss : 230.3061679005623
test_accuracy : 75.82, test_loss : 57.20242664217949, best_accuracy: 79.17

Epoch: 95
tr_accuracy : 80.072, tr_loss : 228.0156146287918
test_accuracy : 77.23, test_loss : 52.68615597486496, best_accuracy: 79.17

Epoch: 96
tr_accuracy : 80.154, tr_loss : 227.70980152487755
test_accuracy : 78.21, test_loss : 50.66914847493172, best_accuracy: 79.17

Epoch: 97
tr_accuracy : 79.878, tr_loss : 229.48588013648987
test_accuracy : 76.87, test_loss : 53.45561718940735, best_accuracy: 79.17

Epoch: 98
tr_accuracy : 79.87, tr_loss : 229.71799817681313
test_accuracy : 75.56, test_loss : 58.33381986618042, best_accuracy: 79.17

Epoch: 99
tr_accuracy : 80.022, tr_loss : 227.10776022076607
test_accuracy : 75.91, test_loss : 56.2998241186142, best_accuracy: 79.17

Epoch: 100
tr_accuracy : 80.076, tr_loss : 228.3729848265648
test_accuracy : 74.52, test_loss : 60.79788076877594, best_accuracy: 79.17

Epoch: 101
tr_accuracy : 80.268, tr_loss : 224.04698407649994
test_accuracy : 78.9, test_loss : 49.08011344075203, best_accuracy: 79.17

Epoch: 102
tr_accuracy : 80.16, tr_loss : 227.2917682826519
test_accuracy : 78.95, test_loss : 48.3091225028038, best_accuracy: 79.17

Epoch: 103
tr_accuracy : 80.5, tr_loss : 223.42831248044968
test_accuracy : 77.36, test_loss : 53.510207802057266, best_accuracy: 79.17

Epoch: 104
tr_accuracy : 80.448, tr_loss : 222.85486456751823
test_accuracy : 75.94, test_loss : 56.6504901945591, best_accuracy: 79.17

Epoch: 105
tr_accuracy : 80.416, tr_loss : 223.06981629133224
test_accuracy : 75.84, test_loss : 57.45225524902344, best_accuracy: 79.17

Epoch: 106
tr_accuracy : 80.22, tr_loss : 225.8053159713745
test_accuracy : 77.39, test_loss : 52.57344776391983, best_accuracy: 79.17

Epoch: 107
tr_accuracy : 80.718, tr_loss : 221.62116223573685
test_accuracy : 77.93, test_loss : 50.6064296066761, best_accuracy: 79.17

Epoch: 108
tr_accuracy : 80.71, tr_loss : 220.09554693102837
test_accuracy : 78.54, test_loss : 50.85491597652435, best_accuracy: 79.17

Epoch: 109
tr_accuracy : 80.404, tr_loss : 221.46779236197472
test_accuracy : 76.01, test_loss : 56.92654237151146, best_accuracy: 79.17

Epoch: 110
tr_accuracy : 80.678, tr_loss : 221.6148402094841
test_accuracy : 75.79, test_loss : 59.280923902988434, best_accuracy: 79.17

Epoch: 111
tr_accuracy : 80.692, tr_loss : 223.2709108889103
test_accuracy : 76.36, test_loss : 55.103475511074066, best_accuracy: 79.17

Epoch: 112
tr_accuracy : 81.132, tr_loss : 217.87625151872635
test_accuracy : 76.01, test_loss : 57.51938897371292, best_accuracy: 79.17

Epoch: 113
tr_accuracy : 80.74, tr_loss : 218.73542320728302
test_accuracy : 77.83, test_loss : 51.7506787776947, best_accuracy: 79.17

Epoch: 114
tr_accuracy : 81.082, tr_loss : 216.94705826044083
test_accuracy : 77.78, test_loss : 51.947078227996826, best_accuracy: 79.17

Epoch: 115
tr_accuracy : 81.346, tr_loss : 214.3132876753807
test_accuracy : 78.99, test_loss : 50.418415039777756, best_accuracy: 79.17

Epoch: 116
tr_accuracy : 80.984, tr_loss : 217.33235961198807
test_accuracy : 74.5, test_loss : 59.359084367752075, best_accuracy: 79.17

Epoch: 117
tr_accuracy : 81.172, tr_loss : 213.46020659804344
test_accuracy : 78.6, test_loss : 50.155124336481094, best_accuracy: 79.17

Epoch: 118
tr_accuracy : 81.094, tr_loss : 216.01455795764923
test_accuracy : 78.16, test_loss : 50.58155843615532, best_accuracy: 79.17

Epoch: 119
tr_accuracy : 81.228, tr_loss : 214.00508925318718
test_accuracy : 78.04, test_loss : 51.31787295639515, best_accuracy: 79.17

Epoch: 120
tr_accuracy : 81.426, tr_loss : 212.753300011158
test_accuracy : 75.81, test_loss : 56.25212435424328, best_accuracy: 79.17

Epoch: 121
tr_accuracy : 81.554, tr_loss : 210.62561652064323
test_accuracy : 78.61, test_loss : 49.41120579838753, best_accuracy: 79.17

Epoch: 122
tr_accuracy : 81.274, tr_loss : 212.29682928323746
Saving..
test_accuracy : 79.22, test_loss : 48.84897854924202, best_accuracy: 79.22

Epoch: 123
tr_accuracy : 81.414, tr_loss : 211.99538359045982
test_accuracy : 78.6, test_loss : 49.233949452638626, best_accuracy: 79.22

Epoch: 124
tr_accuracy : 81.588, tr_loss : 210.13947838544846
test_accuracy : 77.2, test_loss : 51.98716700077057, best_accuracy: 79.22

Epoch: 125
tr_accuracy : 81.548, tr_loss : 210.49399384856224
test_accuracy : 77.17, test_loss : 53.64852228760719, best_accuracy: 79.22

Epoch: 126
tr_accuracy : 81.69, tr_loss : 208.2207961678505
Saving..
test_accuracy : 79.4, test_loss : 47.35815033316612, best_accuracy: 79.4

Epoch: 127
tr_accuracy : 81.736, tr_loss : 208.27555933594704
test_accuracy : 77.58, test_loss : 51.94751864671707, best_accuracy: 79.4

Epoch: 128
tr_accuracy : 81.61, tr_loss : 210.54204180836678
test_accuracy : 75.34, test_loss : 58.06326359510422, best_accuracy: 79.4

Epoch: 129
tr_accuracy : 81.94, tr_loss : 205.5162116587162
test_accuracy : 78.82, test_loss : 49.30153131484985, best_accuracy: 79.4

Epoch: 130
tr_accuracy : 81.932, tr_loss : 205.1346205174923
test_accuracy : 76.32, test_loss : 55.627834141254425, best_accuracy: 79.4

Epoch: 131
tr_accuracy : 82.264, tr_loss : 204.92764231562614
test_accuracy : 78.3, test_loss : 50.65284302830696, best_accuracy: 79.4

Epoch: 132
tr_accuracy : 82.116, tr_loss : 203.69064006209373
test_accuracy : 78.33, test_loss : 49.044749706983566, best_accuracy: 79.4

Epoch: 133
tr_accuracy : 82.182, tr_loss : 204.18038466572762
test_accuracy : 78.33, test_loss : 50.90455695986748, best_accuracy: 79.4

Epoch: 134
tr_accuracy : 82.208, tr_loss : 203.4308375120163
test_accuracy : 78.69, test_loss : 51.85327151417732, best_accuracy: 79.4

Epoch: 135
tr_accuracy : 82.464, tr_loss : 201.14688962697983
Saving..
test_accuracy : 80.11, test_loss : 45.4848013818264, best_accuracy: 80.11

Epoch: 136
tr_accuracy : 82.236, tr_loss : 201.79455775022507
Saving..
test_accuracy : 81.03, test_loss : 42.711446553468704, best_accuracy: 81.03

Epoch: 137
tr_accuracy : 82.438, tr_loss : 199.17838788032532
test_accuracy : 78.12, test_loss : 50.91014540195465, best_accuracy: 81.03

Epoch: 138
tr_accuracy : 82.278, tr_loss : 201.2776294350624
test_accuracy : 75.7, test_loss : 57.14188653230667, best_accuracy: 81.03

Epoch: 139
tr_accuracy : 82.596, tr_loss : 198.52043026685715
test_accuracy : 79.08, test_loss : 49.73654493689537, best_accuracy: 81.03

Epoch: 140
tr_accuracy : 82.416, tr_loss : 200.27657586336136
test_accuracy : 78.29, test_loss : 51.280292093753815, best_accuracy: 81.03

Epoch: 141
tr_accuracy : 82.516, tr_loss : 197.94699515402317
test_accuracy : 77.84, test_loss : 52.97491291165352, best_accuracy: 81.03

Epoch: 142
tr_accuracy : 82.724, tr_loss : 198.16149008274078
test_accuracy : 79.9, test_loss : 46.39227253198624, best_accuracy: 81.03

Epoch: 143
tr_accuracy : 82.95, tr_loss : 193.8207072019577
test_accuracy : 79.36, test_loss : 49.07028737664223, best_accuracy: 81.03

Epoch: 144
tr_accuracy : 83.008, tr_loss : 193.71983286738396
test_accuracy : 80.46, test_loss : 46.873200595378876, best_accuracy: 81.03

Epoch: 145
tr_accuracy : 83.016, tr_loss : 194.26423877477646
test_accuracy : 79.16, test_loss : 48.926170378923416, best_accuracy: 81.03

Epoch: 146
tr_accuracy : 83.242, tr_loss : 193.01911997795105
test_accuracy : 80.7, test_loss : 45.47200360894203, best_accuracy: 81.03

Epoch: 147
tr_accuracy : 83.114, tr_loss : 191.94079956412315
test_accuracy : 79.13, test_loss : 49.84061995148659, best_accuracy: 81.03

Epoch: 148
tr_accuracy : 83.338, tr_loss : 190.89584305882454
test_accuracy : 77.9, test_loss : 52.093825697898865, best_accuracy: 81.03

Epoch: 149
tr_accuracy : 83.452, tr_loss : 188.64248457551003
test_accuracy : 79.99, test_loss : 46.83841699361801, best_accuracy: 81.03

Epoch: 150
tr_accuracy : 83.474, tr_loss : 187.83166646957397
Saving..
test_accuracy : 81.47, test_loss : 42.10868647694588, best_accuracy: 81.47

Epoch: 151
tr_accuracy : 83.262, tr_loss : 190.54648214578629
test_accuracy : 75.41, test_loss : 57.330368876457214, best_accuracy: 81.47

Epoch: 152
tr_accuracy : 83.628, tr_loss : 186.70505943894386
Saving..
test_accuracy : 81.5, test_loss : 43.65116825699806, best_accuracy: 81.5

Epoch: 153
tr_accuracy : 83.716, tr_loss : 186.31154361367226
test_accuracy : 79.66, test_loss : 46.99451279640198, best_accuracy: 81.5

Epoch: 154
tr_accuracy : 83.768, tr_loss : 185.10953310132027
test_accuracy : 78.6, test_loss : 52.88541367650032, best_accuracy: 81.5

Epoch: 155
tr_accuracy : 83.636, tr_loss : 185.99715474247932
test_accuracy : 79.32, test_loss : 48.36577385663986, best_accuracy: 81.5

Epoch: 156
tr_accuracy : 83.644, tr_loss : 185.73646508157253
test_accuracy : 81.26, test_loss : 44.20786854624748, best_accuracy: 81.5

Epoch: 157
tr_accuracy : 83.782, tr_loss : 182.86530116200447
test_accuracy : 79.82, test_loss : 46.58252337574959, best_accuracy: 81.5

Epoch: 158
tr_accuracy : 83.912, tr_loss : 182.54530760645866
test_accuracy : 81.31, test_loss : 42.84525153040886, best_accuracy: 81.5

Epoch: 159
tr_accuracy : 83.834, tr_loss : 180.91633531451225
test_accuracy : 81.49, test_loss : 43.5134799182415, best_accuracy: 81.5

Epoch: 160
tr_accuracy : 84.35, tr_loss : 178.16345125436783
test_accuracy : 79.5, test_loss : 48.70591202378273, best_accuracy: 81.5

Epoch: 161
tr_accuracy : 84.3, tr_loss : 178.62182772159576
Saving..
test_accuracy : 81.99, test_loss : 42.64171639084816, best_accuracy: 81.99

Epoch: 162
tr_accuracy : 84.446, tr_loss : 178.42349095642567
test_accuracy : 81.58, test_loss : 43.491606801748276, best_accuracy: 81.99

Epoch: 163
tr_accuracy : 84.698, tr_loss : 175.0093879699707
test_accuracy : 79.61, test_loss : 47.88185304403305, best_accuracy: 81.99

Epoch: 164
tr_accuracy : 84.444, tr_loss : 176.52691432833672
test_accuracy : 80.42, test_loss : 47.90770897269249, best_accuracy: 81.99

Epoch: 165
tr_accuracy : 84.708, tr_loss : 174.89466279745102
test_accuracy : 81.0, test_loss : 45.318677097558975, best_accuracy: 81.99

Epoch: 166
tr_accuracy : 84.588, tr_loss : 175.2035990357399
test_accuracy : 79.48, test_loss : 48.81506824493408, best_accuracy: 81.99

Epoch: 167
tr_accuracy : 84.756, tr_loss : 174.11222054064274
test_accuracy : 79.46, test_loss : 49.64959806203842, best_accuracy: 81.99

Epoch: 168
tr_accuracy : 84.94, tr_loss : 171.72461327910423
test_accuracy : 80.03, test_loss : 46.39120697975159, best_accuracy: 81.99

Epoch: 169
tr_accuracy : 85.124, tr_loss : 170.1009445488453
test_accuracy : 80.53, test_loss : 45.42563447356224, best_accuracy: 81.99

Epoch: 170
tr_accuracy : 85.208, tr_loss : 169.09024612605572
test_accuracy : 80.93, test_loss : 44.95286735892296, best_accuracy: 81.99

Epoch: 171
tr_accuracy : 85.222, tr_loss : 166.97375766932964
test_accuracy : 80.99, test_loss : 44.48100703954697, best_accuracy: 81.99

Epoch: 172
tr_accuracy : 85.204, tr_loss : 168.34414187073708
test_accuracy : 81.34, test_loss : 43.49394562840462, best_accuracy: 81.99

Epoch: 173
tr_accuracy : 85.438, tr_loss : 167.18931540846825
Saving..
test_accuracy : 82.69, test_loss : 40.849327236413956, best_accuracy: 82.69

Epoch: 174
tr_accuracy : 85.722, tr_loss : 163.72247311472893
test_accuracy : 80.1, test_loss : 47.76089885830879, best_accuracy: 82.69

Epoch: 175
tr_accuracy : 85.58, tr_loss : 165.192174077034
test_accuracy : 80.83, test_loss : 45.043940514326096, best_accuracy: 82.69

Epoch: 176
tr_accuracy : 85.866, tr_loss : 160.85866586863995
test_accuracy : 81.73, test_loss : 42.32220196723938, best_accuracy: 82.69

Epoch: 177
tr_accuracy : 85.82, tr_loss : 161.98953613638878
test_accuracy : 78.46, test_loss : 54.359802424907684, best_accuracy: 82.69

Epoch: 178
tr_accuracy : 85.826, tr_loss : 160.64507327973843
Saving..
test_accuracy : 82.78, test_loss : 39.70227926969528, best_accuracy: 82.78

Epoch: 179
tr_accuracy : 85.914, tr_loss : 159.75746288895607
test_accuracy : 79.89, test_loss : 48.5141761302948, best_accuracy: 82.78

Epoch: 180
tr_accuracy : 86.23, tr_loss : 157.4574286788702
test_accuracy : 82.0, test_loss : 42.844099283218384, best_accuracy: 82.78

Epoch: 181
tr_accuracy : 86.094, tr_loss : 157.33638866245747
test_accuracy : 81.83, test_loss : 41.28667336702347, best_accuracy: 82.78

Epoch: 182
tr_accuracy : 86.072, tr_loss : 157.87125377357006
test_accuracy : 81.69, test_loss : 43.42016391456127, best_accuracy: 82.78

Epoch: 183
tr_accuracy : 86.362, tr_loss : 156.00711235404015
test_accuracy : 79.35, test_loss : 49.566583424806595, best_accuracy: 82.78

Epoch: 184
tr_accuracy : 86.608, tr_loss : 151.88142697513103
test_accuracy : 81.34, test_loss : 44.5205729752779, best_accuracy: 82.78

Epoch: 185
tr_accuracy : 86.44, tr_loss : 153.35016816854477
test_accuracy : 82.34, test_loss : 42.82207462191582, best_accuracy: 82.78

Epoch: 186
tr_accuracy : 86.494, tr_loss : 152.14577807486057
test_accuracy : 82.52, test_loss : 41.58647233247757, best_accuracy: 82.78

Epoch: 187
tr_accuracy : 86.614, tr_loss : 151.42950923740864
test_accuracy : 79.77, test_loss : 48.15030878782272, best_accuracy: 82.78

Epoch: 188
tr_accuracy : 86.854, tr_loss : 149.29546023905277
test_accuracy : 81.82, test_loss : 43.643699914216995, best_accuracy: 82.78

Epoch: 189
tr_accuracy : 87.144, tr_loss : 145.71276542544365
test_accuracy : 81.73, test_loss : 44.342833787202835, best_accuracy: 82.78

Epoch: 190
tr_accuracy : 87.11, tr_loss : 145.97949887812138
test_accuracy : 81.23, test_loss : 45.916600316762924, best_accuracy: 82.78

Epoch: 191
tr_accuracy : 87.31, tr_loss : 144.24226394295692
test_accuracy : 81.49, test_loss : 43.37199321389198, best_accuracy: 82.78

Epoch: 192
tr_accuracy : 87.416, tr_loss : 144.28952857851982
test_accuracy : 82.62, test_loss : 41.12601798772812, best_accuracy: 82.78

Epoch: 193
tr_accuracy : 87.42, tr_loss : 141.27765715122223
test_accuracy : 81.85, test_loss : 44.29352185130119, best_accuracy: 82.78

Epoch: 194
tr_accuracy : 87.58, tr_loss : 141.62011675536633
Saving..
test_accuracy : 85.12, test_loss : 38.13582092523575, best_accuracy: 85.12

Epoch: 195
tr_accuracy : 87.642, tr_loss : 139.3073755055666
test_accuracy : 82.32, test_loss : 41.57683503627777, best_accuracy: 85.12

Epoch: 196
tr_accuracy : 87.72, tr_loss : 139.47611187398434
test_accuracy : 82.04, test_loss : 43.0760600566864, best_accuracy: 85.12

Epoch: 197
tr_accuracy : 87.732, tr_loss : 138.4773467183113
test_accuracy : 82.68, test_loss : 41.36654958128929, best_accuracy: 85.12

Epoch: 198
tr_accuracy : 88.172, tr_loss : 135.32024301588535
test_accuracy : 83.63, test_loss : 38.99009756743908, best_accuracy: 85.12

Epoch: 199
tr_accuracy : 87.974, tr_loss : 135.39542324841022
test_accuracy : 82.47, test_loss : 42.053930819034576, best_accuracy: 85.12
==============================================================================
Running epilogue script on indigo52.

Submit time  : 2021-09-10T14:08:09
Start time   : 2021-09-10T14:08:10
End time     : 2021-09-10T15:08:31
Elapsed time : 01:00:21 (Timelimit=12:00:00)

Job ID: 468613
Cluster: i5
User/Group: jz1n20/fp
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 02:34:28
CPU Efficiency: 25.60% of 10:03:30 core-walltime
Job Wall-clock time: 01:00:21
Memory Utilized: 9.85 GB
Memory Efficiency: 0.00% of 0.00 MB

